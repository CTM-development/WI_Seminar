---
title: "ACME Case Study"
author: "Team 14"
date: "30.7.2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
require(dplyr)
require(broom)
require(ggplot2)
require(cluster)
library("tidyverse")
library(dplyr)

# using pacman to laod and install any packages
install.packages("pacman")

# load packages
pacman::p_load(pacman, tidyverse, rio, dplyr, ggplot2, plotly, tidyr, rmarkdown, tinytex, lubridate, gridExtra, scales)

knitr::opts_chunk$set(echo = TRUE)
```

Lucca Baumg√§rtner (2269532)  
Christopher Minderlein (22484855)  
Theresa

# Introduction

ACME is a manufacturer for premium quality blenders "made in germany". Their unique selling point is the user experience, provided by the touch screens on their devices. High customer satisfaction is seen as the key to customer loyalty and should be achieved through communication and kind service. This is why all devices come with a five year warranty by default. In case of a malfunction, the customers can visit the local servicepoint in Nuremberg, where the device will be taken in for repair.

Surveys have shown that the current service process is far from optimal, resulting in a lower customer satisfaction. In order to improve the process, we will perform an extensive analysis of the current process, identify the weaknesses and bottlenecks and provide suggestions for improvement.

The basis for this analysis is an event log, provided by ACME. The logfile will be analyzed using the programming language R and the process analytics software Celonis.

# Process analytics

In this section we will perform analyses on the provided dataset. First we have to ensure the integrity and consistency of the data, otherwise the results cannot be trusted. After cleaning and preparing the data, we will extract relevant information about the process and meaningful conferences from it. We'll do this by applying basic aggregation and visualization techniques but also through the use of unsupervised machine learning and the powerful process analytics suite Celonis.

## Information gathering

First we'll have to get a general overview of the dataset, its dimensions and attributes.

```{r events, include=FALSE}
event_log <- read_delim("./data/event_log.csv", delim=";")
```

```{r summary}
summary(event_log)

```
This summary provides us with basic information about the attributes, saved for each event:

- CASE_ID (char)
- ACTIVITY (char)
- TIMESTAMP (datetime)
- REPAIR_IN_TIME_5D (int/boolean)
- DEVICETYPE (char)
- SERVICEPOINT (char)

To get a better idea of what the events look like, we output the first ten rows of the dataset.
```{r head}
head(event_log, n=10)

```

Of course this is only a small example of possible events. We want to know about all possible values contained in a certain column. Doing this, gives us a better idea of how many different devices or servicepoints there are, but also helps identifying possibly invalid values.
```{r uniques}
unique(event_log$ACTIVITY)
unique(event_log$DEVICETYPE)
unique(event_log$SERVICEPOINT)
unique(event_log$REPAIR_IN_TIME_5D)
```

From this output we can already identify two attributes that could cause problems later on in our analyses. It seems like some events didn't properly log the devicetype and the servicepoint, resulting in NA-values.

## Data cleaning

We know that there are Exclude logs without information about the servicepoint or the device type.

```{r na omit}
clean_events <- na.omit(event_log)
```

Create a subset of corrupt logs for further investigation.

```{r corrupted}
corrupted_events <- subset(event_log,is.na(DEVICETYPE) | is.na(SERVICEPOINT))
```


```{r chris data motification, echo=FALSE}
# cleaning the data and doing some tests
df_clean_logs <- as.data.frame(do.call(cbind, clean_events))

df_clean_logs$TIMESTAMP <- as.POSIXct(clean_events$TIMESTAMP, tz="",format="%Y-%m-%d %H:%M:%OS", optional = FALSE)
```

```{r creating df_mod_logs (a modificated copy of the base dataset), echo=FALSE}
df_mod_logs <- df_clean_logs

# creating DATE column (calculating for each row)
df_mod_logs$DATE <- with(df_clean_logs, as.Date(TIMESTAMP))

# creating WEEKDAY column
df_mod_logs$WEEKDAY <- weekdays(df_mod_logs$DATE)

# those cases started in 2013 and ended in 2018
sleeper_cases <- c("Case5304", "Case5502", "Case5544")

df_sleeper_cases <- df_clean_logs[df_mod_logs$CASE_ID%in%sleeper_cases,]
df_mod_logs <- df_mod_logs[!(df_clean_logs$CASE_ID%in%sleeper_cases),]

# checking if some case ids have datapoints with mixed rit values (rit = repair in time 5 days)
df_check_for_corrupt_repair <- df_mod_logs %>% group_by(CASE_ID) %>% summarise(ACTIVITY_COUNT = n(),
                                                                             RIT = first(REPAIR_IN_TIME_5D),
                                                                             RIT_GROUP_COUNTER = (table(REPAIR_IN_TIME_5D))[2],
                                                                             ERROR = !((RIT == 0 & 0 == RIT_GROUP_COUNTER) | (RIT ==1 & RIT_GROUP_COUNTER == ACTIVITY_COUNT))
                                                                             )
```

#### creating a new dataframe containing aggregated information per case_id

```{r create_new_dataframe_case_id_aggregats, echo=TRUE}
case_id_aggregated_information <- 
  df_mod_logs %>% group_by(CASE_ID) %>% 
  summarise(SERVICEPOINT = first(SERVICEPOINT),
            DEVICETYPE = first(DEVICETYPE), 
            ACTIVITY_COUNT = n(),
            START_DATETIME = min(TIMESTAMP), 
            END_DATETIME = max(TIMESTAMP),
            RIT = first(REPAIR_IN_TIME_5D), 
            THROUGHPUT_TIME_HOURS = 
              as.numeric(difftime(END_DATETIME, START_DATETIME, units="hours")),
            START_MONTH = month(START_DATETIME),
            START_YEAR = year(START_DATETIME),
)
# order dataset based on START_DATETIME
case_id_aggregated_information <- case_id_aggregated_information[order(case_id_aggregated_information$START_DATETIME, decreasing = FALSE),]
            
rit_cases_too_long <- case_id_aggregated_information[case_id_aggregated_information$RIT == 1 & case_id_aggregated_information$THROUGHPUT_TIME_HOURS>120,]
```

### new dataframe containing duration information of each activity log

```{r df creation, echo=True}
df_activity_information <- 
  df_mod_logs %>% 
  group_by(CASE_ID, ACTIVITY) %>%
  summarise(
    START_TS = min(TIMESTAMP)
  )

calc_duration = function(x){
  current_activity <- x[2]
  current_case_id <- x[1]
  current_activity_ts <- as.POSIXct(x[3], tz="UTC",format="%Y-%m-%d %H:%M:%OS", optional = FALSE)
  
  # print(paste("current case id: ", current_case_id))
  # print(paste("current activity: " , current_activity))
  # print(paste("current activity ts: " , current_activity_ts))
  
  ts_df <- as.data.frame(select(df_mod_logs[df_mod_logs$CASE_ID == current_case_id & df_mod_logs$ACTIVITY != current_activity, ], ACTIVITY, TIMESTAMP))
  # renaming
  names(ts_df)[names(ts_df)=="ACTIVITY"] <- "NEXT_ACTIVITY"
  names(ts_df)[names(ts_df)=="TIMESTAMP"] <- "NEXT_ACTIVITY_TS"
  
  # print(ts_df$TIMESTAMP)
  
  ts_df <- ts_df[(difftime(ts_df$NEXT_ACTIVITY_TS, current_activity_ts)>0), ]
  
  ts_df[order(ts_df$NEXT_ACTIVITY_TS), ]
  
  # print(ts_df$TIMESTAMP)
  # print(ts_df$ACTIVITY)
  
  re <- ts_df[1:1, ]
  re$ACTIVITY <- current_activity
  re$CASE_ID <- current_case_id
  
  return(re)
}

re_list <- apply(df_activity_information, 1 , calc_duration)
print("apply done")
re_df <- as.data.frame(do.call(rbind, re_list)) 

merge_columns <- c("CASE_ID", "ACTIVITY")
df_activity_information <- merge(df_activity_information, re_df, by=merge_columns, all.x = TRUE)

df_activity_information$DURATION = difftime(df_activity_information$NEXT_ACTIVITY_TS, df_activity_information$START_TS, units = "hours")

```

### new dataframe containg throughput-time summaries per month

```{r tp-time per month, echo=TRUE}
# grp by month
tp_time_monthly_avg <- 
  case_id_aggregated_information %>% 
  group_by(START_MONTH, START_YEAR) %>% 
  summarise(
    TP_TIME_MEAN = mean(THROUGHPUT_TIME_HOURS),
    TP_TIME_MEDIAN = median(THROUGHPUT_TIME_HOURS),
    CASE_COUNT = n()
  )
```

## Data analytics

### Number of unique service cases
```{r unique cases}
unique(clean_events$CASE_ID) %>% length()
```

### Distribution of events among servicepoints
```{r sp-distro}
# servicepoints
servicepoints <- table(clean_events$SERVICEPOINT)

sp_df = as.data.frame(servicepoints) %>% rename(Servicepoint = Var1)

sp_distro_plot <- ggplot(data=sp_df, aes(x = Servicepoint, y= Freq, label= Freq)) +
  geom_bar(stat="identity") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))

sp_distro_plot
```

### "Five-day-repair quota" by servicepoint
```{r 5-day-quota}

# das reperaturzeit flag steht in jedem eintrag eines CASEs, uns langt aber ein 
# Eintrag je CASE
distinct_cases <- distinct(clean_events, CASE_ID, .keep_all = TRUE)

# nach Servicepoint gruppieren und die flags aufsummieren
by_servicep <- group_by(distinct_cases, SERVICEPOINT) %>%
  summarise(fivedaysum = sum(REPAIR_IN_TIME_5D), all = n())
# quote berechnen mit anzahl der "schnellen" CASEs
by_servicep$quota = by_servicep$fivedaysum / by_servicep$all * 100

# plot zeichnen
plot <- ggplot(data = by_servicep, aes(x=SERVICEPOINT, y=quota, label=all)) +
  geom_bar(stat="identity") + ylab("\"In time\" repair quota in %")+
  xlab("Servicepoint") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))
plot
```

### Clusteranalysis of device robustness

ACME has a product portfolio of many different devices. We assume that these devices differ in processing quality, proneness to malfunction and longevity. The following cluster analysis aims to identify the most robust products and the most fragile ones, clustering them in quality groups. We define robustness through two factors: The total number of times that the observed device was brought in for service and secondly, the average time it took to repair it. This means that, per our definition, the most robust devices are the ones rarely brought in for service and when they are, their repair only takes a short amount of time.

```{r robustness}
case_total_duration <- group_by(clean_events, CASE_ID) %>%
  group_by(DEVICETYPE, .add = TRUE) %>%
  summarise(timemax = max(TIMESTAMP), timemin = min(TIMESTAMP), duration = timemax - timemin)

avg_duration_by_device <- case_total_duration %>%
  group_by(DEVICETYPE) %>%
  summarize(case_count = n(), avg_time = mean(duration))

avg_duration_by_device$avg_time <- as.numeric(avg_duration_by_device$avg_time, units="days")

ggplot(avg_duration_by_device, aes(case_count, avg_time)) + geom_point()
```

After performing the aggregations, needed to calculate the average repair time and total case count per device, we can display how the devices are distributed in these two dimensions. In the plot above, each dot represents a device. The closer the dot is to the top right, the less robust is the device.

Before performing a clusteranalysis, one has to determine the number of clusters that should be created from the data. Creating a dendrogram first, can help to decide what number of clusters makes sense.

```{r dendro, fig.width = 10}
h.cluster <- avg_duration_by_device %>% dist(., method = "euclidean") %>% hclust(., method="ward.D")
plot(h.cluster)

```

The dendrogram shows that the ideal number of clusters is between 2-3. With this information we proceed to create the final clusters.

```{r cluster}

p.cluster <- avg_duration_by_device %>% select(avg_time, case_count) %>% kmeans(., 3)
p.cluster$cluster <- as.factor(p.cluster$cluster)

ggplot(avg_duration_by_device, aes(case_count, avg_time, label = DEVICETYPE)) + 
  scale_fill_discrete(name = "Cluster") + 
  geom_label(aes(fill = p.cluster$cluster), colour = "white", fontface = "bold", size = 2)
```


## Celonis

Note: Cases before the year 2018 were erased as in previous analytics because first ones documented were in 2013, but marked with throughput times of 5 years and some missing entries. From 2014 to 2017, there were no cases.

"Happy Path" + average throughput time per month:
This "Happy Path" in the picture shows an optimal sequence of activities. It takes about 24 days to process, all paths (without extreme outliers) got a 17 days average duration. Including these outliers, the  average throughput time is at 20 days (compare picture 3: Bottlenecks). The Happy Path involves the activities Creation, DeviceReceived, Letter, Transmission, Approved, NoteHotline, InDelivery and Completed in this order.
Throughput time was on its maximum in October 2018 (24 days) and took 20 days in January 2019, February 2019 and April 2019, which is the second highest duration.
![Happy Path + throughput time per month](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/HappyPath+DurationPerMonth.jpg?raw=true)
Happy Path is the most common path.
![Most common path](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/Most%20Common%20Path.PNG?raw=true)


"Happy Path" + average case number per day in one month:
The case count was highest in October 2018 (103 cases), February 2019 (107), March 2019 (102) and April 2019 (103).
Regarding the previous celonis analysis, the average throughput time per month and the average amount of cases per day in one month can be put in relation to each other, the more cases per day, the longer the throughput times.
![Happy Path + average case number per day in one month](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/HappyPath+CasesPerDay.jpg?raw=true)

Bottlenecks:
The Bottleneck Analysis reveals exceedingly time consuming activities.
![Bottlenecks](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/Bottlenecks.jpg?raw=true)




# Business implications

## Optimizing load distribution and working steps
Currently the distribution of the workload among the servicepoints is suboptimal. The servicepoints E and L account for the majority of events, while A - D barely logged anything. This leads us to the assumption that most of the service cases are being processed by those SPs. We expect that an equal distribution of the workload between all available SPs would lead to an increase in repair speed and total capacity.

Due to the relation of average throughput time per month and the average amount of cases per day in one month, as seen in the first two celonis analytics, we recommend working faster, optionally with additional employees or overtime hours, instead of slowlier while more devices are sent in.

Some of the activity connections in the Bottleneck Analysis from celonis which cost 1 day or more to process should be revised, especially the ones with throughput time increase of 5 days or more. 
For example, 10% of the steps from NoteWorkshop to InDelivery can take the maximum process delay of 10 days. Due to this, the packing and sending of the devices should be sped up. Other relevant consecutive activities to improve are DeviceReceived->InDelivery, DeviceReceived->Letter, NoteHotline->InDelivery, which take 5 or more days and affect 25% or more each. Often, there is a delay after the activity DeviceReceived, so it should be investigated why further actions such as Transmission or Letter take an unnecessarily long time to be executed.
Creation->DeviceReceived with 54% of affected cases and 6 days of delay can't be altered since the customer is in charge to send the device after the service task creation and the delivery time can't be influenced.


## Error-prone devices
Different devices have a different proneness to technical failure. The underlying assumption is that those devices which are most likely to fail, will be brought in for service the most often. Also the devices differ in how long it takes on average to repair them. Using these two factors, one can estimate how service and resource intensive these devices are during their lifetime.

The cluster analysis shows which devices are the most resource intensive ones by dividing them into three groups. The most error-prone device, with over 3000 service cases in the observed timespan and an average repair time of 27 days is the model AB52. We suggest removing this device from the product portfolio, as frequent failure of the device has a negative impact on customer satisfaction. 


## Improve data collection
In order to perform detailed analysis of the service process, more data about the timing of activities is needed. Currently only the completion time of an activity is logged. We recommend to also track the start time of each activity. This way it's possible to make statements about the real duration of the activity and possible idle-times. As of now, it's impossible to say if an activity took a lot of time or if there was a lot of idle-time before it started.
Also there were some event logs without information about the associated servicepoint. We recommend to install mechanisms that prevent incomplete logs from being saved.


