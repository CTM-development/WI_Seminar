---
title: "ACME Case Study"
author: "Team 14"
date: "4 6 2021"
output: pdf_document
---

```{r setup, include=FALSE}
require(dplyr)
require(broom)
require(ggplot2)
require(cluster)
library("tidyverse")
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

# Process analytics

## Information gathering

Datensatz lesen und generellen Überblick verschaffen.

```{r events, include=FALSE}
event_log <- read_delim("./data/event_log.csv", delim=";")
```
Zusammenfassung ausgeben
```{r summary}
summary(event_log)

```

Ersten 10 Datensätzen ausgeben
```{r head}
head(event_log, n=10)

```

Wertebereich für interessante Spalten ausgeben
```{r uniques}
unique(event_log$ACTIVITY)
unique(event_log$DEVICETYPE)
unique(event_log$SERVICEPOINT)
unique(event_log$REPAIR_IN_TIME_5D)
```

## Data cleaning

Datensätze ohne Angabe zu Servicepoint oder Gerät ausschließen.

```{r na omit}
clean_events <- na.omit(event_log)
```

Datensatz aus unvollständigen Sätzen abspalten.
```{r corrupted}
corrupted_events <- subset(event_log,is.na(DEVICETYPE) | is.na(SERVICEPOINT))
```
## Data analytics

Wie viele verschiedene Bearbeitungsfälle gibt es?

```{r unique cases}
unique(clean_events$CASE_ID) %>% length()
```
Wie sind die Events auf die Servicepoints verteilt?
```{r sp-distro}
# servicepoints
servicepoints <- table(clean_events$SERVICEPOINT)

sp_df = as.data.frame(servicepoints) %>% rename(Servicepoint = Var1)

sp_distro_plot <- ggplot(data=sp_df, aes(x = Servicepoint, y= Freq, label= Freq)) +
  geom_bar(stat="identity") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))

sp_distro_plot
```
5-Tage-Reperatur-Quote nach Servicepoint

```{r 5-day-quota}

# das reperaturzeit flag steht in jedem eintrag eines CASEs, uns langt aber ein 
# Eintrag je CASE
distinct_cases <- distinct(clean_events, CASE_ID, .keep_all = TRUE)

# nach Servicepoint gruppieren und die flags aufsummieren
by_servicep <- group_by(distinct_cases, SERVICEPOINT) %>%
  summarise(fivedaysum = sum(REPAIR_IN_TIME_5D), all = n())
# quote berechnen mit anzahl der "schnellen" CASEs
by_servicep$quota = by_servicep$fivedaysum / by_servicep$all * 100

# plot zeichnen
plot <- ggplot(data = by_servicep, aes(x=SERVICEPOINT, y=quota, label=all)) +
  geom_bar(stat="identity") + ylab("\"In time\" repair quota in %")+
  xlab("Servicepoint") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))
plot
```
Clusteranalyse Anfälligkeit der Geräte
```{r cluster}
case_total_duration <- group_by(clean_events, CASE_ID) %>%
  group_by(DEVICETYPE, .add = TRUE) %>%
  summarise(timemax = max(TIMESTAMP), timemin = min(TIMESTAMP), duration = timemax - timemin)

avg_duration_by_device <- case_total_duration %>%
  group_by(DEVICETYPE) %>%
  summarize(case_count = n(), avg_time = mean(duration))

avg_duration_by_device$avg_time <- as.numeric(avg_duration_by_device$avg_time, units="days")

ggplot(avg_duration_by_device, aes(case_count, avg_time)) + geom_point()

h.cluster <- avg_duration_by_device %>% dist(., method = "euclidean") %>% hclust(., method="ward.D")
plot(h.cluster)

p.cluster <- avg_duration_by_device %>% select(avg_time, case_count) %>% kmeans(., 3)
p.cluster$cluster <- as.factor(p.cluster$cluster)

ggplot(avg_duration_by_device, aes(case_count, avg_time, label = DEVICETYPE)) + 
  scale_fill_discrete(name = "Cluster") + 
  geom_label(aes(fill = p.cluster$cluster), colour = "white", fontface = "bold", size = 2)
```

## Celonis





# Business implications

## Optimizing load distribution

Currently the distribution of the workload among the servicepoints is suboptimal. The servicepoints E and L account for the majority of events, while A - D barely logged anything. This leads us to the assumption that most of the service cases are being processed by those SPs. We expect that an equal distribution of the workload between all available SPs would lead to an increase in repair speed and total capacity.

## Error-prone devices

Different devices have a different proneness to technical failure. The underlying assumption is that those devices which are most likely to fail, will be brought in for service the most often. Also the devices differ in how long it takes on average to repair them. Using these two factors, one can estimate how service and resource intensive these devices are during their lifetime.

The cluster analysis shows which devices are the most resource intensive ones by dividing them into three groups. The most error-prone device, with over 3000 service cases in the observed timespan and an average repair time of 27 days is the model AB52. We suggest removing this device from the product portfolio, as frequent failure of the device has a negative impact on customer satisfaction. 

## Improve date collection

In order to perform detailed analysis of the service process, more data about the timing of activities is needed. Currently only the completion time of an activity is logged. We recommend to also track the start time of each activity. This way it's possible to make statements about the real duration of the activity and possible idle-times. As of now, it's impossible to say if an activity took a lot of time or if there was a lot of idle-time before it started.


