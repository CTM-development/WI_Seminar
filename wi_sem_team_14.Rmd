---
title: "ACME Case Study"
author: "Team 14"
date: "4 6 2021"
output: pdf_document
---

```{r setup, include=FALSE}
require(dplyr)
require(broom)
require(ggplot2)
require(cluster)
library("tidyverse")
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

# Process analytics

## Information gathering

Datensatz lesen und generellen Überblick verschaffen.

```{r events, include=FALSE}
event_log <- read_delim("./data/event_log.csv", delim=";")
```
Zusammenfassung ausgeben
```{r summary}
summary(event_log)

```

Ersten 10 Datensätzen ausgeben
```{r head}
head(event_log, n=10)

```

Wertebereich für interessante Spalten ausgeben
```{r uniques}
unique(event_log$ACTIVITY)
unique(event_log$DEVICETYPE)
unique(event_log$SERVICEPOINT)
unique(event_log$REPAIR_IN_TIME_5D)
```

## Data cleaning

Datensätze ohne Angabe zu Servicepoint oder Gerät ausschließen.

```{r na omit}
clean_events <- na.omit(event_log)
```

Datensatz aus unvollständigen Sätzen abspalten.
```{r corrupted}
corrupted_events <- subset(event_log,is.na(DEVICETYPE) | is.na(SERVICEPOINT))
```
## Data analytics

Wie viele verschiedene Bearbeitungsfälle gibt es?

```{r unique cases}
unique(clean_events$CASE_ID) %>% length()
```
Wie sind die Events auf die Servicepoints verteilt?
```{r sp-distro}
# servicepoints
servicepoints <- table(clean_events$SERVICEPOINT)

sp_df = as.data.frame(servicepoints) %>% rename(Servicepoint = Var1)

sp_distro_plot <- ggplot(data=sp_df, aes(x = Servicepoint, y= Freq, label= Freq)) +
  geom_bar(stat="identity") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))

sp_distro_plot
```
5-Tage-Reperatur-Quote nach Servicepoint

```{r 5-day-quota}

# das reperaturzeit flag steht in jedem eintrag eines CASEs, uns langt aber ein 
# Eintrag je CASE
distinct_cases <- distinct(clean_events, CASE_ID, .keep_all = TRUE)

# nach Servicepoint gruppieren und die flags aufsummieren
by_servicep <- group_by(distinct_cases, SERVICEPOINT) %>%
  summarise(fivedaysum = sum(REPAIR_IN_TIME_5D), all = n())
# quote berechnen mit anzahl der "schnellen" CASEs
by_servicep$quota = by_servicep$fivedaysum / by_servicep$all * 100

# plot zeichnen
plot <- ggplot(data = by_servicep, aes(x=SERVICEPOINT, y=quota, label=all)) +
  geom_bar(stat="identity") + ylab("\"In time\" repair quota in %")+
  xlab("Servicepoint") +
  geom_text(size = 3, position = position_stack(vjust = 0.5))
plot
```
Clusteranalyse Anfälligkeit der Geräte
```{r cluster}
case_total_duration <- group_by(clean_events, CASE_ID) %>%
  group_by(DEVICETYPE, .add = TRUE) %>%
  summarise(timemax = max(TIMESTAMP), timemin = min(TIMESTAMP), duration = timemax - timemin)

avg_duration_by_device <- case_total_duration %>%
  group_by(DEVICETYPE) %>%
  summarize(case_count = n(), avg_time = mean(duration))

avg_duration_by_device$avg_time <- as.numeric(avg_duration_by_device$avg_time, units="days")

ggplot(avg_duration_by_device, aes(case_count, avg_time)) + geom_point()

h.cluster <- avg_duration_by_device %>% dist(., method = "euclidean") %>% hclust(., method="ward.D")
plot(h.cluster)

p.cluster <- avg_duration_by_device %>% select(avg_time, case_count) %>% kmeans(., 3)
p.cluster$cluster <- as.factor(p.cluster$cluster)

ggplot(avg_duration_by_device, aes(case_count, avg_time, label = DEVICETYPE)) + 
  scale_fill_discrete(name = "Cluster") + 
  geom_label(aes(fill = p.cluster$cluster), colour = "white", fontface = "bold", size = 2)
```


## Celonis

Note: Cases before year 2018 were erased as in other analytics because first cases documented were in 2013, but marked with throughput times of 5 years and missing entries.

"Happy Path" + throughput time per month:
This "Happy Path" in the picture shows an optimal sequence of activities. It takes an average of 24 days, all paths got 17 days average duration. 
throughput time was on its maximum in October 2018 (24 days) and took 20 days in January, February and April, the second highest duration
![Happy Path + throughput time per month](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/HappyPath+DurationPerMonth.jpg?raw=true)

"Happy Path" + average case number per day in one month:
case count was highest in October 2018 (103 cases), February 2019 (107), March 2019 (102) and April 2019 (103)
regarding the previous celonis analysis the throughput time per month and the average of cases per day in one month can be seen in relation to each other
![Happy Path + average case number per day in one month](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/HappyPath+CasesPerDay.jpg?raw=true)

Bottlenecks:
Some of the activity connections which cost 1 day or more to process should be revised, especially the ones with throughput time increase of 5 days or more.
For example, 10% of the steps from NoteWorkshop to InDelivery can take the maximum process delay of 10 days. Due to this, the packing and sending of the devices should be sped up.
Other relevant consecutive activities to improve are DeviceReceived->InDelivery, DeviceReceived->Letter, NoteHotline->InDelivery, which take 5 or more days and affect 25% or more.
Creation->DeviceReceived with 54% of affected cases and 6 delay days can't be altered since the customer is in charge to send the device after the service task creation and the delivery time can't be influenced.
![Bottlenecks](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/Bottlenecks.jpg?raw=true)

Path with shortest throughput time:
![Path with shortest throughput time](https://github.com/CTM-development/WI_Seminar/blob/main/celonis_imgs/Path%20with%20shortest%20Throughput%20Time.PNG?raw=true)

Top 10 paths with the shortest throughput times:
no improvements needed
conforms 0% of all paths
![Top10 shortest throughput time](https://github.com/CTM-development/WI_Seminar/blob/main/Top%2010%20shortest%20Throughput%20Time.jpg?raw=true)

Top 10 paths with the longest throughput times:
should be improved
![top10 longest throughput time](https://github.com/CTM-development/WI_Seminar/blob/main/Top%2010%20longest%20Throughput%20Time.jpg?raw=true)

Top 10 most common paths:
conform to 21% of all possible paths
![top10 most common](https://github.com/CTM-development/WI_Seminar/blob/main/Top10%20Most%20Common.jpg?raw=true)





# Business implications

## Optimizing load distribution

Currently the distribution of the workload among the servicepoints is suboptimal. The servicepoints E and L account for the majority of events, while A - D barely logged anything. This leads us to the assumption that most of the service cases are being processed by those SPs. We expect that an equal distribution of the workload between all available SPs would lead to an increase in repair speed and total capacity.

## Error-prone devices

Different devices have a different proneness to technical failure. The underlying assumption is that those devices which are most likely to fail, will be brought in for service the most often. Also the devices differ in how long it takes on average to repair them. Using these two factors, one can estimate how service and resource intensive these devices are during their lifetime.

The cluster analysis shows which devices are the most resource intensive ones by dividing them into three groups. The most error-prone device, with over 3000 service cases in the observed timespan and an average repair time of 27 days is the model AB52. We suggest removing this device from the product portfolio, as frequent failure of the device has a negative impact on customer satisfaction. 

## Improve data collection

In order to perform detailed analysis of the service process, more data about the timing of activities is needed. Currently only the completion time of an activity is logged. We recommend to also track the start time of each activity. This way it's possible to make statements about the real duration of the activity and possible idle-times. As of now, it's impossible to say if an activity took a lot of time or if there was a lot of idle-time before it started.
Also there were some event logs without information about the associated servicepoint. We recommend to install mechanisms that prevent incomplete logs from being saved.


